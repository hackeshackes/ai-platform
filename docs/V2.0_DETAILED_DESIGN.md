# AI Platform v2.0 详细设计

## 目录

1. [架构设计](#架构设计)
2. [数据库设计](#数据库设计)
3. [缓存设计](#缓存设计)
4. [任务队列设计](#任务队列设计)
5. [API设计](#api设计)
6. [前端设计](#前端设计)
7. [部署设计](#部署设计)
8. [迁移计划](#迁移计划)

---

## 一、架构设计

### 1.1 整体架构

```
┌─────────────────────────────────────────────────────────────┐
│                      AI Platform v2.0                       │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                   Frontend (React)                   │   │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐  │   │
│  │  │  Dashboard │ │ Projects │ │ Training │ │ Inference │  │   │
│  │  └─────────┘ └─────────┘ └─────────┘ └─────────┘  │   │
│  │         │         │          │          │           │   │
│  └─────────┼─────────┼──────────┼──────────┼───────────┘   │
│            │         │          │          │               │
│            ▼         ▼          ▼          ▼               │
│  ┌─────────────────────────────────────────────────────┐   │
│  │              API Gateway (FastAPI)                    │   │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐  │   │
│  │  │   Auth   │ │  REST   │ │ WebSocket│ │  Admin  │  │   │
│  │  └─────────┘ └─────────┘ └─────────┘ └─────────┘  │   │
│  └─────────────────────────────────────────────────────┘   │
│            │         │          │          │               │
│            ▼         ▼          │          ▼               │
│  ┌─────────────────────────────────────────────────────┐   │
│  │                  Services Layer                      │   │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────┐          │   │
│  │  │ Project   │ │ Training  │ │ Inference │          │   │
│  │  │ Service   │ │ Service  │ │ Service  │          │   │
│  │  └──────────┘ └──────────┘ └──────────┘          │   │
│  └─────────────────────────────────────────────────────┘   │
│            │         │          │          │               │
│     ┌──────┴──────┐  │          │          │               │
│     ▼             ▼  ▼          ▼          ▼               │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐          │
│  │PostgreSQL│ │  Redis  │ │ Celery  │ │ S3/OSS  │          │
│  │  (DB)    │ │ (Cache) │ │ (Queue) │ │ (Storage)│          │
│  └─────────┘ └─────────┘ └─────────┘ └─────────┘          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 技术栈升级

| 层级 | 当前 | v2.0升级 | 收益 |
|------|------|----------|------|
| **数据库** | SQLite | PostgreSQL 15 | 并发/可靠性 |
| **缓存** | 无 | Redis 7 | 性能提升 |
| **任务队列** | 同步 | Celery 5 | 异步处理 |
| **ORM** | SQLAlchemy 2.x | SQLAlchemy 2.x + Alembic | 迁移管理 |
| **前端状态** | Axios | React Query | 缓存管理 |
| **实时通信** | 轮询 | WebSocket | 实时更新 |
| **部署** | 手动 | Docker + K8s | 弹性伸缩 |

### 1.3 微服务划分

| 服务 |职责| 技术 |
|------|------|------|
| **api-gateway** | API入口、认证 | FastAPI |
| **project-service** | 项目管理 | FastAPI + SQLAlchemy |
| **training-service** | 训练任务 | FastAPI + Celery |
| **inference-service** | 推理服务 | FastAPI |
| **gpu-service** | GPU监控 | FastAPI + WebSocket |
| **notification-service** | 通知服务 | FastAPI + WebSocket |

---

## 二、数据库设计

### 2.1 PostgreSQL Schema

```sql
-- 扩展
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pgcrypto";

-- 用户表
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    uuid UUID DEFAULT uuid_generate_v4() UNIQUE NOT NULL,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    password_hash VARCHAR(256) NOT NULL,
    role VARCHAR(20) DEFAULT 'user',
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    last_login_at TIMESTAMP WITH TIME ZONE,
    
    CONSTRAINT users_username_length CHECK (LENGTH(username) >= 3),
    CONSTRAINT users_email_format CHECK (email LIKE '%@%')
);

CREATE INDEX idx_users_uuid ON users(uuid);
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_role ON users(role);

-- 项目表
CREATE TABLE projects (
    id SERIAL PRIMARY KEY,
    uuid UUID DEFAULT uuid_generate_v4() UNIQUE NOT NULL,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    owner_id INTEGER REFERENCES users(id) ON DELETE SET NULL,
    status VARCHAR(50) DEFAULT 'active',
    config JSONB DEFAULT '{}',
    tags JSONB DEFAULT '[]',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    CONSTRAINT projects_name_length CHECK (LENGTH(name) >= 1)
);

CREATE INDEX idx_projects_uuid ON projects(uuid);
CREATE INDEX idx_projects_owner ON projects(owner_id);
CREATE INDEX idx_projects_status ON projects(status);
CREATE INDEX idx_projects_tags ON projects USING GIN(tags);

-- 实验表
CREATE TABLE experiments (
    id SERIAL PRIMARY KEY,
    uuid UUID DEFAULT uuid_generate_v4() UNIQUE NOT NULL,
    project_id INTEGER REFERENCES projects(id) ON DELETE CASCADE,
    name VARCHAR(255) NOT NULL,
    base_model VARCHAR(255),
    task_type VARCHAR(50),
    status VARCHAR(50) DEFAULT 'pending',
    config JSONB DEFAULT '{}',
    metrics JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_experiments_uuid ON experiments(uuid);
CREATE INDEX idx_experiments_project ON experiments(project_id);
CREATE INDEX idx_experiments_status ON experiments(status);

-- 数据集表
CREATE TABLE datasets (
    id SERIAL PRIMARY KEY,
    uuid UUID DEFAULT uuid_generate_v4() UNIQUE NOT NULL,
    project_id INTEGER REFERENCES projects(id) ON DELETE CASCADE,
    name VARCHAR(255) NOT NULL,
    file_path VARCHAR(500),
    size BIGINT DEFAULT 0,
    format VARCHAR(50),
    version VARCHAR(50) DEFAULT 'v1',
    status VARCHAR(50) DEFAULT 'uploaded',
    quality_report JSONB DEFAULT '{}',
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- 数据集版本表
CREATE TABLE dataset_versions (
    id SERIAL PRIMARY KEY,
    uuid UUID DEFAULT uuid_generate_v4() UNIQUE NOT NULL,
    dataset_id INTEGER REFERENCES datasets(id) ON DELETE CASCADE,
    version VARCHAR(20) NOT NULL,
    parent_version_id INTEGER REFERENCES dataset_versions(id),
    commit_message TEXT,
    file_hash VARCHAR(64),
    row_count INTEGER DEFAULT 0,
    file_size BIGINT DEFAULT 0,
    metadata JSONB DEFAULT '{}',
    created_by INTEGER REFERENCES users(id) ON DELETE SET NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- 模型表
CREATE TABLE models (
    id SERIAL PRIMARY KEY,
    uuid UUID DEFAULT uuid_generate_v4() UNIQUE NOT NULL,
    project_id INTEGER REFERENCES projects(id) ON DELETE CASCADE,
    name VARCHAR(255) NOT NULL,
    version VARCHAR(50),
    framework VARCHAR(100),
    file_path VARCHAR(500),
    size BIGINT DEFAULT 0,
    metrics JSONB DEFAULT '{}',
    model_card JSONB DEFAULT '{}',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- 任务表
CREATE TABLE tasks (
    id SERIAL PRIMARY KEY,
    uuid UUID DEFAULT uuid_generate_v4() UNIQUE NOT NULL,
    project_id INTEGER REFERENCES projects(id) ON DELETE SET NULL,
    experiment_id INTEGER REFERENCES experiments(id) ON DELETE SET NULL,
    model_id INTEGER REFERENCES models(id) ON DELETE SET NULL,
    name VARCHAR(255) NOT NULL,
    type VARCHAR(50) NOT NULL,
    status VARCHAR(50) DEFAULT 'pending',
    priority INTEGER DEFAULT 5,
    config JSONB DEFAULT '{}',
    logs TEXT,
    progress FLOAT DEFAULT 0.0,
    gpu_usage JSONB DEFAULT '{}',
    error_message TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE
);

CREATE INDEX idx_tasks_uuid ON tasks(uuid);
CREATE INDEX idx_tasks_project ON tasks(project_id);
CREATE INDEX idx_tasks_status ON tasks(status);
CREATE INDEX idx_tasks_type ON tasks(type);
CREATE INDEX idx_tasks_priority ON tasks(priority DESC);

-- 任务队列表 (Celery)
CREATE TABLE celery_tasks (
    id VARCHAR(36) PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    args JSONB DEFAULT '[]',
    kwargs JSONB DEFAULT '{}',
    status VARCHAR(50) DEFAULT 'PENDING',
    result JSONB,
    traceback TEXT,
    runtime FLOAT,
    worker VARCHAR(100),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    started_at TIMESTAMP WITH TIME ZONE,
    completed_at TIMESTAMP WITH TIME ZONE,
    expires_at TIMESTAMP WITH TIME ZONE
);

CREATE INDEX idx_celery_tasks_status ON celery_tasks(status);
CREATE INDEX idx_celery_tasks_name ON celery_tasks(name);
CREATE INDEX idx_celery_tasks_created ON celery_tasks(created_at);

-- 角色表
CREATE TABLE roles (
    id SERIAL PRIMARY KEY,
    uuid UUID DEFAULT uuid_generate_v4() UNIQUE NOT NULL,
    name VARCHAR(50) UNIQUE NOT NULL,
    description TEXT,
    permissions JSONB NOT NULL DEFAULT '[]',
    is_system BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- 项目权限表
CREATE TABLE project_permissions (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
    project_id INTEGER REFERENCES projects(id) ON DELETE CASCADE,
    role_id INTEGER REFERENCES roles(id) ON DELETE CASCADE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    
    UNIQUE(user_id, project_id)
);

-- 操作审计表
CREATE TABLE audit_logs (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id) ON DELETE SET NULL,
    action VARCHAR(100) NOT NULL,
    resource_type VARCHAR(50),
    resource_id VARCHAR(36),
    details JSONB DEFAULT '{}',
    ip_address INET,
    user_agent TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_audit_logs_user ON audit_logs(user_id);
CREATE INDEX idx_audit_logs_resource ON audit_logs(resource_type, resource_id);
CREATE INDEX idx_audit_logs_created ON audit_logs(created_at);

-- 通知表
CREATE TABLE notifications (
    id SERIAL PRIMARY KEY,
    user_id INTEGER REFERENCES users(id) ON DELETE CASCADE,
    type VARCHAR(50) NOT NULL,
    title VARCHAR(255) NOT NULL,
    message TEXT,
    data JSONB DEFAULT '{}',
    is_read BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_notifications_user ON notifications(user_id);
CREATE INDEX idx_notifications_unread ON notifications(user_id, is_read) WHERE is_read = FALSE;
```

### 2.2 Alembic迁移配置

```python
# alembic/versions/001_init_postgres.py
"""init postgres

Revision ID: 001
Revises: 
Create Date: 2026-09-01

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers
revision = '001'
down_revision = None
branch_labels = None
depends_on = None


def upgrade():
    # 创建users表
    op.create_table(
        'users',
        sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
        sa.Column('uuid', postgresql.UUID(), nullable=False),
        sa.Column('username', sa.String(50), nullable=False),
        sa.Column('email', sa.String(100), nullable=False),
        sa.Column('password_hash', sa.String(256), nullable=False),
        sa.Column('role', sa.String(20), server_default='user', nullable=False),
        sa.Column('is_active', sa.Boolean(), server_default='true', nullable=False),
        sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('CURRENT_TIMESTAMP'), nullable=False),
        sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('CURRENT_TIMESTAMP'), nullable=False),
        sa.Column('last_login_at', sa.DateTime(timezone=True), nullable=True),
        sa.PrimaryKeyConstraint('id'),
        sa.UniqueConstraint('uuid'),
        sa.UniqueConstraint('username'),
        sa.UniqueConstraint('email')
    )
    
    op.create_index('idx_users_uuid', 'users', ['uuid'], unique=True)
    op.create_index('idx_users_email', 'users', ['email'], unique=True)
    op.create_index('idx_users_role', 'users', ['role'])
    
    # ... 其他表创建


def downgrade():
    op.drop_table('users')
```

---

## 三、缓存设计

### 3.1 Redis缓存策略

```python
# core/cache.py
import redis
from typing import Optional, Any
from datetime import timedelta
import json

class Cache:
    """Redis缓存管理器"""
    
    def __init__(self):
        self.redis = redis.Redis(
            host='localhost',
            port=6379,
            db=0,
            decode_responses=True
        )
    
    # 缓存键前缀
    PREFIX = 'aiplatform:v2:'
    
    # TTL配置
    TTL_CONFIG = {
        'user_session': timedelta(hours=24),
        'project_list': timedelta(minutes=5),
        'task_status': timedelta(seconds=30),
        'gpu_metrics': timedelta(seconds=5),
        'dataset_info': timedelta(hours=1),
        'model_info': timedelta(hours=1),
    }
    
    def _key(self, key: str) -> str:
        return f"{self.PREFIX}{key}"
    
    async def get(self, key: str) -> Optional[Any]:
        """获取缓存"""
        data = self.redis.get(self._key(key))
        return json.loads(data) if data else None
    
    async def set(self, key: str, value: Any, ttl_key: str):
        """设置缓存"""
        self.redis.setex(
            self._key(key),
            self.TTL_CONFIG.get(ttl_key, timedelta(minutes=5)),
            json.dumps(value)
        )
    
    async def delete(self, key: str):
        """删除缓存"""
        self.redis.delete(self._key(key))
    
    async def invalidate_pattern(self, pattern: str):
        """批量删除匹配键"""
        keys = self.redis.keys(self._key(pattern))
        if keys:
            self.redis.delete(*keys)

# 缓存实例
cache = Cache()
```

### 3.2 缓存键设计

| 缓存项 | 键格式 | TTL | 策略 |
|--------|--------|-----|------|
| 用户会话 | `user:{user_id}:session` | 24h | LRU |
| 项目列表 | `user:{user_id}:projects` | 5min | LRU |
| 任务状态 | `task:{task_id}:status` | 30s | LRU |
| GPU指标 | `gpu:{gpu_id}:metrics` | 5s | LRU |
| 数据集信息 | `dataset:{dataset_id}:info` | 1h | LRU |
| 模型信息 | `model:{model_id}:info` | 1h | LRU |

### 3.3 缓存预热

```python
# core/cache_warmer.py
async def warmup_cache():
    """缓存预热"""
    # 预热活跃项目
    projects = await project_service.list_active()
    for project in projects:
        await cache.set(
            f'project:{project.id}',
            project.dict(),
            'project_info'
        )
    
    # 预热系统配置
    config = await settings_service.get_system_config()
    await cache.set('system:config', config.dict(), 'system_config')
```

---

## 四、任务队列设计

### 4.1 Celery配置

```python
# core/celery.py
from celery import Celery
from celery.schedules import crontab

celery_app = Celery(
    'ai_platform',
    broker='redis://localhost:6379/1',
    backend='redis://localhost:6379/2'
)

# Celery配置
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='Asia/Shanghai',
    enable_utc=True,
    
    # 任务配置
    task_track_started=True,
    task_time_limit=3600,  # 1小时
    task_soft_time_limit=3000,  # 50分钟
    task_acks_late=True,
    task_reject_on_worker_lost=True,
    
    # 重试配置
    task_default_retry_delay=60,
    max_retries=3,
    
    # Worker配置
    worker_prefetch_multiplier=4,
    worker_concurrency=4,
    
    # 任务路由
    task_routes={
        'tasks.training.*': {'queue': 'training'},
        'tasks.inference.*': {'queue': 'inference'},
        'tasks.data.*': {'queue': 'data'},
        'tasks.notify.*': {'queue': 'notification'},
    },
    
    # 定时任务
    beat_schedule={
        'sync-gpu-metrics': {
            'task': 'tasks.monitoring.sync_gpu',
            'schedule': timedelta(seconds=5),
            'queue': 'monitoring',
        },
        'cleanup-tasks': {
            'task': 'tasks.maintenance.cleanup',
            'schedule': crontab(hour=3, minute=0),
            'queue': 'maintenance',
        },
    },
)
```

### 4.2 任务定义

```python
# tasks/training.py
from celery import shared_task
from celery.exceptions import SoftTimeLimitExceeded

@shared_task(
    bind=True,
    name='tasks.training.submit',
    queue='training',
    max_retries=3,
    default_retry_delay=60
)
def submit_training_task(self, project_id: int, config: dict):
    """提交训练任务"""
    try:
        # 创建Celery任务记录
        task_id = self.request.id
        
        # 更新任务状态
        update_task_status(task_id, 'RUNNING')
        
        # 执行训练
        result = run_training(project_id, config)
        
        # 更新结果
        update_task_result(task_id, result)
        
        return {'status': 'success', 'task_id': task_id}
        
    except SoftTimeLimitExceeded:
        update_task_status(task_id, 'FAILED', error='任务超时')
        raise
    except Exception as e:
        update_task_status(task_id, 'FAILED', error=str(e))
        raise self.retry(exc=e)


@shared_task(
    bind=True,
    name='tasks.training.monitor',
    queue='monitoring'
)
def monitor_training_progress(self, task_id: int):
    """监控训练进度"""
    progress = get_training_progress(task_id)
    
    # 更新Redis缓存
    cache.set(f'task:{task_id}:progress', progress, 'task_progress')
    
    # 通过WebSocket推送到前端
    websocket_manager.broadcast({
        'type': 'training_progress',
        'task_id': task_id,
        'progress': progress
    })
    
    return progress


@shared_task(
    name='tasks.inference.run',
    queue='inference'
)
def run_inference(model_id: int, prompt: str, config: dict):
    """执行推理"""
    result = inference_engine.predict(model_id, prompt, **config)
    return result
```

### 4.3 任务优先级

```python
# 任务优先级枚举
class TaskPriority:
    CRITICAL = 0  # 紧急
    HIGH = 1      # 高
    NORMAL = 5    # 普通
    LOW = 9       # 低

# 使用示例
@shared_task(priority=TaskPriority.HIGH)
def high_priority_task():
    pass
```

---

## 五、API设计

### 5.1 RESTful API结构

```
/api/v2
├── /auth
│   ├── POST /token
│   ├── POST /refresh
│   ├── GET /me
│   └── POST /logout
│
├── /projects
│   ├── GET / (列表)
│   ├── POST / (创建)
│   ├── GET /{id} (详情)
│   ├── PUT /{id} (更新)
│   └── DELETE /{id} (删除)
│
├── /experiments
│   ├── GET /
│   ├── POST /
│   ├── GET /{id}
│   └── POST /{id}/start
│
├── /datasets
│   ├── GET /
│   ├── POST / (上传)
│   ├── GET /{id}
│   └── GET /{id}/versions
│
├── /training
│   ├── GET /models (可用模型)
│   ├── GET /templates (训练模板)
│   ├── POST /submit (提交任务)
│   ├── GET /jobs (任务列表)
│   └── GET /jobs/{id}/status (状态)
│
├── /inference
│   ├── GET /models
│   └── POST /generate
│
├── /gpu
│   └── GET /status
│
├── /metrics
│   └── GET /loss
│
├── /users
│   ├── GET /
│   ├── POST /
│   └── GET /{id}
│
├── /roles
│   ├── GET /
│   └── POST /
│
└── /admin
    ├── GET /stats
    └── POST /migrate
```

### 5.2 WebSocket API

```python
# WebSocket端点
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket_manager.connect(websocket)
    
    try:
        while True:
            data = await websocket.receive_json()
            
            if data["type"] == "subscribe":
                await websocket_manager.subscribe(
                    websocket,
                    data["channel"],
                    data.get("resource_id")
                )
            
            elif data["type"] == "training_progress":
                await websocket_manager.broadcast_to_channel(
                    "training",
                    data["task_id"],
                    data["progress"]
                )
    
    except WebSocketDisconnect:
        websocket_manager.disconnect(websocket)
```

### 5.3 API响应格式

```python
# 响应模型
class APIResponse(BaseModel):
    success: bool
    data: Optional[Any] = None
    error: Optional[str] = None
    meta: Optional[Dict] = None

class PaginatedResponse(APIResponse):
    data: List[Any]
    meta: {
        "total": int,
        "page": int,
        "limit": int,
        "has_more": bool
    }

# 使用示例
@router.get("/projects")
async def list_projects(
    page: int = Query(1, ge=1),
    limit: int = Query(20, ge=1, le=100),
    db: Session = Depends(get_db)
) -> PaginatedResponse:
    projects, total = await project_service.paginate(page, limit)
    
    return PaginatedResponse(
        success=True,
        data=projects,
        meta={
            "total": total,
            "page": page,
            "limit": limit,
            "has_more": page * limit < total
        }
    )
```

---

## 六、前端设计

### 6.1 React Query集成

```typescript
// hooks/useProjects.ts
import { useQuery, useMutation, useQueryClient } from '@tanstack/react-query';
import { api } from '../api/client';

// 查询键
export const projectKeys = {
  all: ['projects'] as const,
  lists: () => [...projectKeys.all, 'list'] as const,
  list: (filters: ProjectFilters) => [...projectKeys.lists(), filters] as const,
  details: () => [...projectKeys.all, 'detail'] as const,
  detail: (id: string) => [...projectKeys.details(), id] as const,
};

// 获取项目列表
export function useProjects(filters: ProjectFilters) {
  return useQuery({
    queryKey: projectKeys.list(filters),
    queryFn: () => api.projects.list(filters),
    staleTime: 5 * 60 * 1000, // 5分钟
  });
}

// 创建项目
export function useCreateProject() {
  const queryClient = useQueryClient();
  
  return useMutation({
    mutationFn: (data: CreateProjectDTO) => api.projects.create(data),
    onSuccess: () => {
      queryClient.invalidateQueries({ queryKey: projectKeys.lists() });
    },
  });
}

// 实时GPU状态
export function useGPUMetrics() {
  return useQuery({
    queryKey: ['gpu', 'metrics'],
    queryFn: () => api.gpu.status(),
    refetchInterval: 5000, // 5秒轮询
  });
}
```

### 6.2 WebSocket连接

```typescript
// hooks/useWebSocket.ts
import { useEffect, useState, useCallback } from 'react';

export function useWebSocket() {
  const [connected, setConnected] = useState(false);
  const [lastMessage, setLastMessage] = useState<WSMessage | null>(null);
  
  const ws = useRef<WebSocket | null>(null);
  
  useEffect(() => {
    ws.current = new WebSocket('ws://localhost:8000/ws');
    
    ws.current.onopen = () => setConnected(true);
    ws.current.onclose = () => setConnected(false);
    
    ws.current.onmessage = (event) => {
      const data = JSON.parse(event.data);
      setLastMessage(data);
    };
    
    return () => {
      ws.current?.close();
    };
  }, []);
  
  const subscribe = useCallback((channel: string, resourceId?: string) => {
    ws.current?.send(JSON.stringify({
      type: 'subscribe',
      channel,
      resource_id: resourceId,
    }));
  }, []);
  
  return { connected, lastMessage, subscribe };
}
```

### 6.3 组件架构

```
src/
├── components/
│   ├── common/              # 通用组件
│   │   ├── Button/
│   │   ├── Card/
│   │   ├── Table/
│   │   └── Modal/
│   │
│   ├── layout/              # 布局组件
│   │   ├── Header/
│   │   ├── Sidebar/
│   │   └── Layout/
│   │
│   └── features/            # 功能组件
│       ├── ProjectCard/
│       ├── TaskProgress/
│       ├── GPUChart/
│       └── LossCurve/
│
├── pages/                    # 页面
│   ├── Dashboard/
│   ├── Projects/
│   ├── Training/
│   └── Inference/
│
├── hooks/                    # 自定义Hook
│   ├── useProjects/
│   ├── useTasks/
│   ├── useWebSocket/
│   └── usePermissions/
│
├── services/                # 服务层
│   ├── api/
│   ├── websocket/
│   └── auth/
│
├── stores/                   # 状态管理
│   ├── userStore/
│   └── projectStore/
│
└── utils/                    # 工具函数
    ├── formatters/
    └── validators/
```

---

## 七、部署设计

### 7.1 Docker Compose配置

```yaml
# docker-compose.yml
version: '3.8'

services:
  # PostgreSQL
  postgres:
    image: postgres:15-alpine
    container_name: ai-platform-db
    environment:
      POSTGRES_DB: ai_platform
      POSTGRES_USER: aiplatform
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U aiplatform -d ai_platform"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis
  redis:
    image: redis:7-alpine
    container_name: ai-platform-cache
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # API服务
  api:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: ai-platform-api
    environment:
      - DATABASE_URL=postgresql://aiplatform:${DB_PASSWORD}@postgres:5432/ai_platform
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "8000:8000"

  # Celery Worker
  worker:
    build:
      context: ./backend
      dockerfile: Dockerfile.worker
    container_name: ai-platform-worker
    command: celery -A core.celery worker -Q training,inference,data -l info
    environment:
      - DATABASE_URL=postgresql://aiplatform:${DB_PASSWORD}@postgres:5432/ai_platform
      - CELERY_BROKER=redis://redis:6379/1
    depends_on:
      - postgres
      - redis

  # Celery Beat
  beat:
    build:
      context: ./backend
      dockerfile: Dockerfile.worker
    container_name: ai-platform-beat
    command: celery -A core.celery beat -l info
    depends_on:
      - redis

  # 前端
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: ai-platform-frontend
    ports:
      - "3000:80"
    depends_on:
      - api

volumes:
  postgres_data:
  redis_data:
```

### 7.2 Kubernetes部署

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-platform-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-platform-api
  template:
    metadata:
      labels:
        app: ai-platform-api
    spec:
      containers:
      - name: api
        image: ai-platform/api:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: ai-platform-secrets
              key: database-url
        - name: REDIS_URL
          valueFrom:
            configMapKeyRef:
              name: ai-platform-config
              key: redis-url
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: ai-platform-api
spec:
  selector:
    app: ai-platform-api
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP
```

### 7.3 CI/CD流水线

```yaml
# .github/workflows/ci.yml
name: CI/CD

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: ai_platform_test
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
        ports:
          - 5432:5432
      redis:
        image: redis:7
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest pytest-cov
    
    - name: Run tests
      run: |
        pytest --cov=app --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Build and push Docker images
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: |
          ai-platform/api:${{ github.sha }}
          ai-platform/frontend:${{ github.sha }}
```

---

## 八、迁移计划

### 8.1 SQLite到PostgreSQL迁移

```python
# scripts/migrate_sqlite_to_postgres.py
import sqlite3
from sqlalchemy import create_engine
import pandas as pd

def migrate():
    # 读取SQLite数据
    sqlite_conn = sqlite3.connect('ai_platform.db')
    users = pd.read_sql_query('SELECT * FROM users', sqlite_conn)
    projects = pd.read_sql_query('SELECT * FROM projects', sqlite_conn)
    # ... 其他表
    
    # 写入PostgreSQL
    pg_engine = create_engine('postgresql://user:pass@localhost:5432/ai_platform')
    
    users.to_sql('users', pg_engine, if_exists='append', index=False)
    projects.to_sql('projects', pg_engine, if_exists='append', index=False)
    # ... 其他表
    
    # 迁移UUID
    pg_engine.execute('UPDATE users SET uuid = uuid_generate_v4() WHERE uuid IS NULL')
```

### 8.2 迁移步骤

| 步骤 | 内容 | 风险 | 回滚方案 |
|------|------|------|----------|
| 1 | 备份SQLite数据 | 低 | 保留备份 |
| 2 | 创建PostgreSQL Schema | 低 | 删除重建 |
| 3 | 双写阶段 | 中 | 回滚到SQLite |
| 4 | 数据迁移 | 中 | 重试迁移 |
| 5 | 切换读取源 | 高 | 快速回滚 |
| 6 | 清理SQLite | 低 | 保留观察期 |

---

**文档版本**: 1.0.0  
**创建日期**: 2026-02-09  
**状态**: 详细设计完成
